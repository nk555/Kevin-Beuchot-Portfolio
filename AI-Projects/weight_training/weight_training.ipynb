{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\xnive\\anaconda3\\envs\\tf\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\nC:\\Users\\xnive\\anaconda3\\envs\\tf\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\nC:\\Users\\xnive\\anaconda3\\envs\\tf\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "source": [
    "# Getting more than linear estimations by training our weights\n",
    "\n",
    "The idea of this project is to create an architecture that is small and is able to detect non-linear relationships between data. We will first explain this architecture and then make some experiments.\n",
    "\n",
    "The idea behind this architecture is to have a Neural Network, named Task, with inputs I, outputs O and weights W, and instead of regular backpropagation to learn the weights W we implement a second Neural network, named Lift, with inputs I and outputs W (approximations of the weights of the first NN). For training we will consider the \"right\" input to be the weights after a pass of backpropragation, and use MSE to train the network Lift.\n",
    "\n",
    "For this task we will use a simple MLP network for both of our networks but it should be able to extend to any architecture in a trivial way."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Task"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task():\n",
    "    task=tf.keras.Sequential(layers=[tf.keras.layers.Dense(5, activation=\"relu\", input_shape=(3,)), tf.keras.layers.Dense(1)])\n",
    "    task.compile(loss=\"mse\")\n",
    "    return task"
   ]
  },
  {
   "source": [
    "## Lift"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lift():\n",
    "    lift=tf.keras.Sequential(layers=[tf.keras.layers.Dense(10), tf.keras.layers.Dense(26)])\n",
    "    lift.compile(loss=\"mse\")\n",
    "    return lift"
   ]
  },
  {
   "source": [
    "## Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lifter(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(lifter, self).__init__()\n",
    "        self.task=tf.keras.Sequential(layers=[tf.keras.layers.Dense(5, activation=\"relu\", input_shape=(3,)), tf.keras.layers.Dense(1)])\n",
    "        self.lift=tf.keras.Sequential(layers=[tf.keras.layers.Dense(10, activation=\"relu\", input_shape=(3,)), tf.keras.layers.Dense(26)])\n",
    "        self.task.compile(loss=\"mse\")\n",
    "        self.lift.compile(loss=\"mse\")\n",
    "        weights=self.task.get_weights()\n",
    "        self.weight_shapes=[np.shape(weight) for weight in weights]\n",
    "\n",
    "    def call(self, X):\n",
    "        pred=np.array([[]])\n",
    "        for i in range(len(X)):\n",
    "            new_weights=self.lift(X[i:i+1]).numpy()\n",
    "            j=0\n",
    "            start=0\n",
    "            end=0\n",
    "            end2=0\n",
    "            for layer in self.task.layers:\n",
    "                end+=get_size(self.weight_shapes[j])\n",
    "                end2=end+get_size(self.weight_shapes[j+1])\n",
    "                new_layer_weights=[np.reshape(new_weights[:,start:end], self.weight_shapes[j]), np.reshape(new_weights[:,end:end2], self.weight_shapes[j+1])]\n",
    "                layer.set_weights(new_layer_weights)\n",
    "                start=end2\n",
    "                end=end2\n",
    "                j+=2\n",
    "            new_pred=self.task(X[i:i+1]).numpy()\n",
    "            pred=np.concatenate([pred, new_pred], axis=1)\n",
    "        return pred\n",
    "\n",
    "    def fit(self, X,y, epochs=1, print_every=1000):\n",
    "        for j in range(epochs):\n",
    "            for i in range(len(X)):\n",
    "                if (i+1)%print_every==0:\n",
    "                    print(i+1)\n",
    "                self.call(X[i:i+1])\n",
    "                self.task.fit(X[i:i+1],y[i:i+1], verbose=0)\n",
    "                weights=flatten_weights(self.task.get_weights())\n",
    "                self.lift.fit(X[i:i+1], weights, verbose=0)\n",
    "        "
   ]
  },
  {
   "source": [
    "## Auxiliary Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_weights(weights):\n",
    "    flat=np.array([])\n",
    "    for weight in weights:\n",
    "        flat=np.concatenate([flat, weight.flatten()])\n",
    "    return np.array([flat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size(shape):\n",
    "    size=1\n",
    "    for i in shape:\n",
    "        size*=i\n",
    "    return size"
   ]
  },
  {
   "source": [
    "# Data and Training\n",
    "\n",
    "We will make a small artificial task where the data has inputs (x,y,z) and outputs x^2+2yz"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "X=np.array([[random.randrange(0,10), random.randrange(0,10), random.randrange(0,10)] for i in range(10000)])\n",
    "y=np.array([X[i,0]**2+2*X[i,1]*X[i,2] for i in range(10000)])\n",
    "\n",
    "X_test=np.array([[random.randrange(0,10), random.randrange(0,10), random.randrange(0,10)] for i in range(10000)])\n",
    "y_test=np.array([X_test[i,0]**2+2*X_test[i,1]*X_test[i,2] for i in range(10000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "model=lifter()\n",
    "model.fit(X, y, epochs=1, print_every=1000)"
   ]
  },
  {
   "source": [
    "# Comparison with regular model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1=tf.keras.Sequential(layers=[tf.keras.layers.Dense(5, activation=\"relu\", input_shape=(3,)), tf.keras.layers.Dense(1)])\n",
    "test2=tf.keras.Sequential(layers=[tf.keras.layers.Dense(10, activation=\"relu\", input_shape=(3,)), tf.keras.layers.Dense(10, activation=\"relu\"), tf.keras.layers.Dense(1)])\n",
    "test1.compile(loss=\"mse\")\n",
    "test2.compile(loss=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10000/10000 [==============================] - 4s 369us/step - loss: 3712.5261\n",
      "10000/10000 [==============================] - 4s 381us/step - loss: 1909.2088\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a939b26108>"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "test1.fit(X,y, batch_size=1)\n",
    "test2.fit(X,y, batch_size=1)"
   ]
  },
  {
   "source": [
    "# Let's compare with the test set."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "11.751248024717423\n577.8288923440286\n299.96931042965923\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "print(mse(y_test, np.reshape(model.call(X_test), -1)))\n",
    "print(mse(y_test, test1(X_test)))\n",
    "print(mse(y_test, test2(X_test)))"
   ]
  },
  {
   "source": [
    "## Our model is clearly much better\n",
    "\n",
    "However, there are a few things to consider, training time is much longer, however this can probably be optimized. We could also implement some ideas for batches that make computations faster by paralelization. \n",
    "\n",
    "To be fair, this model was designed for this sort of task in mind, but in general it can be used to look for different interactions between the variables that are non-linear. And although activation functions do this and are able to model many classes of functions it is interesting to see what can be found if we do not know much about our variables and task at hand.\n",
    "\n",
    "Let us compare what happens if we did some more "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/9\n",
      "10000/10000 [==============================] - 4s 368us/step - loss: 440.9614\n",
      "Epoch 2/9\n",
      "10000/10000 [==============================] - 4s 375us/step - loss: 256.1920\n",
      "Epoch 3/9\n",
      "10000/10000 [==============================] - 4s 386us/step - loss: 170.3760\n",
      "Epoch 4/9\n",
      "10000/10000 [==============================] - 4s 368us/step - loss: 142.9517\n",
      "Epoch 5/9\n",
      "10000/10000 [==============================] - 4s 367us/step - loss: 134.8003\n",
      "Epoch 6/9\n",
      "10000/10000 [==============================] - 4s 364us/step - loss: 131.7325\n",
      "Epoch 7/9\n",
      "10000/10000 [==============================] - 4s 364us/step - loss: 130.1595\n",
      "Epoch 8/9\n",
      "10000/10000 [==============================] - 4s 366us/step - loss: 128.7534\n",
      "Epoch 9/9\n",
      "10000/10000 [==============================] - 4s 376us/step - loss: 127.4233\n",
      "Epoch 1/9\n",
      "10000/10000 [==============================] - 4s 398us/step - loss: 138.7394\n",
      "Epoch 2/9\n",
      "10000/10000 [==============================] - 4s 382us/step - loss: 72.7631\n",
      "Epoch 3/9\n",
      "10000/10000 [==============================] - 4s 409us/step - loss: 51.4276\n",
      "Epoch 4/9\n",
      "10000/10000 [==============================] - 4s 419us/step - loss: 36.0963\n",
      "Epoch 5/9\n",
      "10000/10000 [==============================] - 4s 421us/step - loss: 24.1819\n",
      "Epoch 6/9\n",
      "10000/10000 [==============================] - 4s 390us/step - loss: 17.8581\n",
      "Epoch 7/9\n",
      "10000/10000 [==============================] - 4s 390us/step - loss: 13.5702\n",
      "Epoch 8/9\n",
      "10000/10000 [==============================] - 4s 386us/step - loss: 11.0141\n",
      "Epoch 9/9\n",
      "10000/10000 [==============================] - 4s 387us/step - loss: 9.2805\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a92ddbef48>"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "test1.fit(X,y, batch_size=1, epochs=9)\n",
    "test2.fit(X,y, batch_size=1, epochs=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "125.23955670380508\n12.815976485974884\n"
     ]
    }
   ],
   "source": [
    "print(mse(y_test, test1(X_test)))\n",
    "print(mse(y_test, test2(X_test)))"
   ]
  },
  {
   "source": [
    "## But what happens with a test set outside of the domain, in this case out of range(0,10)?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test2=np.array([[random.randrange(100,200), random.randrange(100,200), random.randrange(100,200)] for i in range(10000)])\n",
    "y_test2=np.array([X_test2[i,0]**2+2*X_test2[i,1]*X_test2[i,2] for i in range(10000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "450406552376.2903\n4308421073.077956\n4121513846.260282\n"
     ]
    }
   ],
   "source": [
    "print(mse(y_test2, np.reshape(model.call(X_test2), -1)))\n",
    "print(mse(y_test2, test1(X_test2)))\n",
    "print(mse(y_test2, test2(X_test2)))"
   ]
  },
  {
   "source": [
    "## After seeing that with bigger numbers the error is bigger than that of the bigger MLP and when it gets even bigger it even gets worse than the smaller one, I expect that the error is growing because we are multiplying two errors, one for the prediction of weights and then for the net. I would then expect that with more training we would see bigger improvements on our model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X,Y, epochs=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mse(y_test, np.reshape(model.call(X_test), -1)))\n",
    "print(mse(y_test2, np.reshape(model.call(X_test2), -1)))"
   ]
  }
 ]
}